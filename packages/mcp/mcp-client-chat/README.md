# MCP Client Chat

ä¸€ä¸ªåŸºäº TypeScript çš„ Model Context Protocol (MCP) å®¢æˆ·ç«¯å®ç°ï¼Œæ”¯æŒä¸ LLM é›†æˆã€‚

## ç‰¹æ€§

- ğŸš€ **åŒæ¨¡å¼æ”¯æŒ**ï¼šæ”¯æŒä¼ ç»Ÿ REST API å’Œ AI SDK ä¸¤ç§æ¨¡å¼
- ğŸ”§ **å·¥å…·è°ƒç”¨**ï¼šæ”¯æŒ Function Calling å’Œ ReAct ä¸¤ç§ Agent ç­–ç•¥
- ğŸ“¡ **æµå¼å“åº”**ï¼šæ”¯æŒå®æ—¶æµå¼æ•°æ®è¿”å›
- ğŸ”Œ **çµæ´»é…ç½®**ï¼šæ”¯æŒå¤šç§ LLM æä¾›å•†å’Œè‡ªå®šä¹‰ä¼ è¾“å±‚
- ğŸ› ï¸ **MCP é›†æˆ**ï¼šå®Œæ•´çš„ MCP åè®®æ”¯æŒï¼Œå¯è¿æ¥å¤šä¸ª MCP æœåŠ¡å™¨

- **å®‰è£…**

```bash
npm install @opentiny/tiny-agent-mcp-client-chat --save
```

- **å‚æ•°è¯´æ˜**

  - `agentStrategy(string, optional)`: Agent ç­–ç•¥ï¼Œå¯é€‰å€¼ï¼š`'Function Calling'` æˆ– `'ReAct'`ï¼Œé»˜è®¤ä¸º `'Function Calling'`

  - `llmConfig(object)`: LLM é…ç½®é¡¹

    - `useSDK(boolean, optional)`: æ˜¯å¦ä½¿ç”¨ AI SDKï¼Œé»˜è®¤ä¸º false
    - `url(string)`: AI å¹³å° API æ¥å£åœ°å€ï¼Œä½¿ç”¨ AI SDK æ—¶å¯ä¸å¡«
    - `apiKey(string)`: AI å¹³å°çš„ API Key
    - `model(string | LanguageModelV2)`: æ¨¡å‹åç§°æˆ– AI SDK æ¨¡å‹å®ä¾‹
    - `systemPrompt(string)`: ç³»ç»Ÿæç¤ºè¯
    - `summarySystemPrompt(string, optional)`: æ€»ç»“æç¤ºè¯
    - `streamSwitch(boolean, optional)`: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”ï¼Œé»˜è®¤ä¸º true
    - `maxTokens(number, optional)`: æœ€å¤§ç”Ÿæˆ token æ•°
    - `temperature(number, optional)`: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
    - `topP(number, optional)`: Top-p é‡‡æ ·å‚æ•°
    - `topK(number, optional)`: Top-k é‡‡æ ·å‚æ•°
    - `presencePenalty(number, optional)`: å­˜åœ¨æƒ©ç½šå‚æ•°
    - `frequencyPenalty(number, optional)`: é¢‘ç‡æƒ©ç½šå‚æ•°
    - `stopSequences(string[], optional)`: åœæ­¢åºåˆ—
    - `seed(number, optional)`: éšæœºç§å­
    - `maxRetries(number, optional)`: æœ€å¤§é‡è¯•æ¬¡æ•°
    - `abortSignal(AbortSignal, optional)`: ä¸­æ­¢ä¿¡å·
    - `headers(Record<string, string>, optional)`: è‡ªå®šä¹‰è¯·æ±‚å¤´

  - `maxIterationSteps(number)`: Agent æœ€å¤§è¿­ä»£æ­¥æ•°

  - `mcpServersConfig(object)`: mcp-server é…ç½®é¡¹

    - `mcpServers(object)`: mcp-server åŸºç¡€é…ç½®

      - `<serverName(string)>`: mcp-server æœåŠ¡å

        - `url(string)`: mcp-server è¿æ¥åœ°å€
        - `headers(object)`: è¯·æ±‚å¤´
        - `timeout(number)`: è¶…æ—¶æ—¶é•¿
        - `customTransport(Transport | function, optional)`: è‡ªå®šä¹‰ä¼ è¾“å±‚

- **ç¤ºä¾‹**

## REST API ä½¿ç”¨ç¤ºä¾‹

```typescript
import express from "express";
import cors from "cors";
import { createMCPClientChat } from "@opentiny/tiny-agent-mcp-client-chat";

async function main() {
  const app = express();
  app.use(cors());
  app.use(express.json());

  app.post("/chat", async (req, res) => {
    const mcpClientChat = await createMCPClientChat({
      llmConfig: {
        url: "https://openrouter.ai/api/v1/chat/completions",
        apiKey: "<your-api-key>",
        model: "mistralai/mistral-7b-instruct:free",
        systemPrompt: "You are a helpful assistant with access to tools.",
        summarySystemPrompt: "Please provide a brief summary!",
        streamSwitch: true,
        temperature: 0.7,
        maxTokens: 1000,
      },
      maxIterationSteps: 3,
      mcpServersConfig: {
        mcpServers: {
          "localhost-mcp": {
            url: `xxx`,
            headers: {},
            timeout: 60
          },
          "localhost-mcp2": {
            url: `xxx2`,
            headers: {},
            timeout: 60
          },
        },
      },
    });

    try {
      // æµå¼æ•°æ®è¿”å›
      const streamResponse = await mcpClientChat.chat("your question...");

      streamResponse.pipe(res);
    } catch (error) {
      // é”™è¯¯å¤„ç†
    }
  });
}

main();
```

## AI SDK ä½¿ç”¨ç¤ºä¾‹

### ä½¿ç”¨ (OpenAI)[https://ai-sdk.dev/providers/ai-sdk-providers/openai]

```typescript
import { createMCPClientChat } from "@opentiny/tiny-agent-mcp-client-chat";
import { createOpenAI } from '@ai-sdk/openai';

const openai = createOpenAI({
  apiKey: "<your-openai-api-key>", // API key that is being sent using the Authorization header. It defaults to the OPENAI_API_KEY environment variable.
  baseURL: "https://api.openai.com/v1", // Use a different URL prefix for API calls, e.g. to use proxy servers. The default prefix is https://api.openai.com/v1.
  name: "", // The provider name. You can set this when using OpenAI compatible providers to change the model provider property. Defaults to openai.
  organization: "", // OpenAI Organization.
  project: "", // OpenAI project.
  fetch:  (input: RequestInfo, init?: RequestInit) => Promise<Response>, // Custom fetch implementation. Defaults to the global fetch function. You can use it as a middleware to intercept requests, or to provide a custom fetch implementation for e.g. testing.
  headers: { // Custom headers to include in the requests.
    'header-name': 'header-value',
  },
});

const mcpClientChat = await createMCPClientChat({
  llmConfig: {
    useSDK: true, // å¯ç”¨ AI SDK
    url: "https://api.openai.com/v1",
    model: openai("gpt-4o"), // ä½¿ç”¨ AI SDK æ¨¡å‹
    systemPrompt: "You are a helpful assistant with access to tools.",
    streamSwitch: true,
    temperature: 0.7,
    maxTokens: 1000,
  },
  maxIterationSteps: 3,
  mcpServersConfig: {
    mcpServers: {
      "localhost-mcp": {
        url: "http://localhost:3000",
        headers: {},
        timeout: 60
      },
    },
  },
});
```

### ä½¿ç”¨ (DeepSeek)[https://ai-sdk.dev/providers/ai-sdk-providers/deepseek] 

```typescript
import { createMCPClientChat } from "@opentiny/tiny-agent-mcp-client-chat";
import { createDeepSeek } from '@ai-sdk/deepseek';

const deepseek = createDeepSeek({
  apiKey: process.env.DEEPSEEK_API_KEY ?? '',
});

const mcpClientChat = await createMCPClientChat({
  llmConfig: {
    useSDK: true, // å¯ç”¨ AI SDK
    url: "https://api.deepseek.com",
    model: deepseek("deepseek-chat"), // ä½¿ç”¨ AI SDK æ¨¡å‹
    systemPrompt: "You are a helpful assistant with access to tools.",
    streamSwitch: true,
    temperature: 0.6,
    maxTokens: 1500,
  },
  maxIterationSteps: 3,
  mcpServersConfig: {
    mcpServers: {
      "localhost-mcp": {
        url: "http://localhost:3000",
        headers: {},
        timeout: 60
      },
    },
  },
});
```
